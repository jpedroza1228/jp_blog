{
  "hash": "b33c4d195359dc67ffa664431debf12f",
  "result": {
    "markdown": "---\ntitle: \"Tidy Tuesday Coffee Ratings\" \nsubtitle: |\n  I decided to use TidyModels to create some models predicting Washed/Not Washed (Natural) Coffees. \nimage: coffee_process.jpg\ncategories: [Visualizations, Analysis, Tidy Tuesday]\ndate: 2021-05-13\n# citation:\n  # url: \nparams:\n  slug: Tidy-Tuesday-Coffee-Ratings\n  date: 2021-05-13\n---\n\n\n\n\n## Predicting Process of Green Coffee Beans\n\nWith coffee being a hobby of mine, I was scrolling through past Tidy Tuesdays and found one on coffee ratings. Originally I thought looking at predictions of total cup points, but I assumed with all the coffee tasting characteristics that it wouldn't really tell me anything. Instead, I decided to look into the processing method, as there are different taste characteristics between washed and other processing methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoffee <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv') %>% \n  mutate(species = as.factor(species),\n         process = recode(processing_method, \"Washed / Wet\" = \"washed\",\n                          \"Semi-washed / Semi-pulped\" = \"not_washed\",\n                          \"Pulped natural / honey\" = \"not_washed\",\n                          \"Other\" = \"not_washed\",\n                          \"Natural / Dry\" = \"not_washed\",\n                          \"NA\" = NA_character_),\n         process = as.factor(process),\n         process = relevel(process, ref = \"washed\"),\n         country_of_origin = as.factor(country_of_origin)) %>% \n  drop_na(process) %>% \n  filter(country_of_origin != \"Cote d?Ivoire\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1339 Columns: 43\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\nAfter looking at the distributions of procssing methods, I also decided to make the processing method binary with washed and not washed. This worked out better for the prediction models. There are also some descriptives of each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoffee %>% \n  ggplot(aes(processing_method)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncoffee %>% \n  ggplot(aes(process)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\npsych::describe(coffee, na.rm = TRUE)[c(\"n\", \"mean\", \"sd\", \"min\", \"max\", \"skew\", \"kurtosis\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          n    mean      sd   min       max  skew kurtosis\ntotal_cup_points       1168   82.06    2.71 59.83     90.58 -1.95     9.26\nspecies*               1168    1.01    0.09  1.00      2.00 10.65   111.61\nowner*                 1161  133.43   76.78  1.00    287.00  0.10    -1.00\ncountry_of_origin*     1168   14.79   10.08  1.00     36.00  0.31    -1.12\nfarm_name*              883  271.04  154.16  1.00    524.00  0.00    -1.27\nlot_number*             239   93.18   59.86  1.00    202.00  0.18    -1.27\nmill*                   931  196.60  122.71  1.00    419.00  0.24    -1.28\nico_number*            1057  360.89  242.05  1.00    753.00  0.08    -1.27\ncompany*               1077  141.73   76.77  1.00    266.00 -0.10    -1.23\naltitude*              1014  154.54   98.40  1.00    351.00  0.32    -1.04\nregion*                1137  174.91   87.74  1.00    325.00 -0.11    -1.09\nproducer*               995  314.12  175.27  1.00    624.00 -0.04    -1.12\nnumber_of_bags         1168  153.80  130.08  1.00   1062.00  0.37     0.50\nbag_weight*            1168   23.50   16.67  1.00     45.00 -0.23    -1.71\nin_country_partner*    1168    9.68    7.04  1.00     25.00  0.41    -1.31\nharvest_year*          1161    5.82    2.95  1.00     14.00  0.76    -0.46\ngrading_date*          1168  242.39  144.52  1.00    495.00  0.11    -1.21\nowner_1*               1161  135.29   77.68  1.00    290.00  0.09    -1.00\nvariety*               1089   12.61    9.77  1.00     29.00  0.63    -1.24\nprocessing_method*     1168    3.98    1.67  1.00      5.00 -1.13    -0.62\naroma                  1168    7.56    0.31  5.08      8.75 -0.55     4.46\nflavor                 1168    7.51    0.34  6.08      8.83 -0.34     1.73\naftertaste             1168    7.39    0.34  6.17      8.67 -0.45     1.36\nacidity                1168    7.53    0.31  5.25      8.75 -0.30     3.31\nbody                   1168    7.52    0.28  6.33      8.50 -0.10     0.89\nbalance                1168    7.51    0.34  6.08      8.58 -0.10     1.17\nuniformity             1168    9.84    0.50  6.00     10.00 -4.21    20.83\nclean_cup              1168    9.84    0.75  0.00     10.00 -6.98    62.29\nsweetness              1168    9.89    0.52  1.33     10.00 -7.53    80.78\ncupper_points          1168    7.48    0.40  5.17      8.75 -0.64     2.79\nmoisture               1168    0.09    0.05  0.00      0.17 -1.41     0.35\ncategory_one_defects   1168    0.51    2.70  0.00     63.00 14.43   279.42\nquakers                1167    0.17    0.82  0.00     11.00  6.87    57.30\ncolor*                 1070    2.80    0.64  1.00      4.00 -1.51     2.57\ncategory_two_defects   1168    3.79    5.54  0.00     55.00  3.54    18.53\nexpiration*            1168  241.61  144.11  1.00    494.00  0.12    -1.21\ncertification_body*    1168    9.38    6.65  1.00     24.00  0.37    -1.31\ncertification_address* 1168   16.58    7.33  1.00     29.00 -0.19    -1.06\ncertification_contact* 1168    9.39    7.26  1.00     26.00  0.36    -0.96\nunit_of_measurement*   1168    1.87    0.34  1.00      2.00 -2.21     2.87\naltitude_low_meters    1012 1796.86 9073.21  1.00 190164.00 19.36   384.12\naltitude_high_meters   1012 1834.27 9071.86  1.00 190164.00 19.36   384.03\naltitude_mean_meters   1012 1815.56 9072.31  1.00 190164.00 19.36   384.12\nprocess*               1168    1.30    0.46  1.00      2.00  0.86    -1.27\n```\n:::\n:::\n\n\nNow, its time to split the data into training and testing data. I also included the function of `strata` to stratify sampling based on process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05132021)\n\ncoffee_split <- initial_split(coffee, strata = \"process\")\n\ncoffee_train <- training(coffee_split)\ncoffee_test <- testing(coffee_split)\n```\n:::\n\n\nI also did some cross validation for the training dataset and used the metrics I was most interested in.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05132021)\n\ncoffee_fold <- vfold_cv(coffee_train, strata = \"process\", v = 10)\n\nmetric_measure <- metric_set(accuracy, mn_log_loss, roc_auc)\n```\n:::\n\n\nFrom the beginning I was interested in the tasting characteristics and how they would predict whether the green coffee was washed or not washed. I also included the total cup points because I wanted to see the importance of that predictor on the processing method. The only feature engineering I did was to remove any zero variance in the predictors of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05132021)\n\nchar_recipe <- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points,\n                      data = coffee_train) %>% \n  step_zv(all_predictors(), -all_outcomes())\n\nchar_recipe %>% \n  prep() %>% \n  bake(new_data = NULL) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 11\n  aroma flavor aftertaste acidity  body balance unifor~1 clean~2 sweet~3 total~4\n  <dbl>  <dbl>      <dbl>   <dbl> <dbl>   <dbl>    <dbl>   <dbl>   <dbl>   <dbl>\n1  8.17   8.58       8.42    8.42  8.5     8.25       10      10      10    89  \n2  8.08   8.58       8.5     8.5   7.67    8.42       10      10      10    88.2\n3  8.17   8.17       8       8.17  8.08    8.33       10      10      10    87.2\n4  8.42   8.17       7.92    8.17  8.33    8          10      10      10    87.1\n5  8.5    8.5        8       8     8       8          10      10      10    86.9\n6  8      8          8       8.25  8       8.17       10      10      10    86.6\n# ... with 1 more variable: process <fct>, and abbreviated variable names\n#   1: uniformity, 2: clean_cup, 3: sweetness, 4: total_cup_points\n# i Use `colnames()` to see all variable names\n```\n:::\n:::\n\n\n### Logistic Regression\n\nThe first model I wanted to test with the current recipe was logistic regression. The `accuracy` and `roc auc` were alright for a starting model.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'glmnet' was built under R version 4.0.5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'Matrix' was built under R version 4.0.5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lr_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.698    10 0.00342 Preprocessor1_Model1\n2 mn_log_loss binary     0.589    10 0.00775 Preprocessor1_Model1\n3 roc_auc     binary     0.648    10 0.0188  Preprocessor1_Model1\n```\n:::\n:::\n\n\n### Lasso Regression\n\nNow for the first penalized regression. The lasso regression did not improve in either metric. Let's try the next penalized regression.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lasso_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.702    10 0.00397 Preprocessor1_Model1\n2 mn_log_loss binary     0.592    10 0.00850 Preprocessor1_Model1\n3 roc_auc     binary     0.645    10 0.0191  Preprocessor1_Model1\n```\n:::\n:::\n\n\n### Ridge Regression\n\nThe ridge regression was shown to not be a good fitting model. So I tested an additional penalized regression while tuning hyper-parameters.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.rmetrics .cell-code}\ncollect_metrics(ridge_fit)\n```\n:::\n\n\n### Elastic Net Regression\n\nThe elastic net regression had slightly better accuracy than the non-penalized logistic regression but the ROC AUC was exactly the same. While the elastic net regression did not take long computationally due to the small amount of data, this model would not be chosen over the logistic regression.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(elastic_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 300 x 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>       \n 1 0.0000000001        0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n 2 0.0000000001        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso~\n 3 0.0000000001        0 roc_auc     binary     0.648    10 0.0188  Preprocesso~\n 4 0.00000000129       0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n 5 0.00000000129       0 mn_log_loss binary     0.589    10 0.00775 Preprocesso~\n 6 0.00000000129       0 roc_auc     binary     0.648    10 0.0188  Preprocesso~\n 7 0.0000000167        0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n 8 0.0000000167        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso~\n 9 0.0000000167        0 roc_auc     binary     0.648    10 0.0188  Preprocesso~\n10 0.000000215         0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n# ... with 290 more rows\n# i Use `print(n = ...)` to see more rows\n```\n:::\n\n```{.r .cell-code}\nshow_best(elastic_fit, metric = \"accuracy\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 8\n        penalty mixture .metric  .estimator  mean     n std_err .config         \n          <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>           \n1 0.0774          0.111 accuracy binary     0.703    10 0.00434 Preprocessor1_M~\n2 0.0000000001    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n3 0.00000000129   0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n4 0.0000000167    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n5 0.000000215     0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n```\n:::\n\n```{.r .cell-code}\nshow_best(elastic_fit, metric = \"roc_auc\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            \n1 0.0000000001        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n2 0.00000000129       0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n3 0.0000000167        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n4 0.000000215         0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n5 0.00000278          0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n```\n:::\n\n```{.r .cell-code}\nselect_best(elastic_fit, metric = \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  penalty mixture .config               \n    <dbl>   <dbl> <chr>                 \n1  0.0774   0.111 Preprocessor1_Model019\n```\n:::\n\n```{.r .cell-code}\nselect_best(elastic_fit, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n       penalty mixture .config               \n         <dbl>   <dbl> <chr>                 \n1 0.0000000001       0 Preprocessor1_Model001\n```\n:::\n:::\n\n\n### New Recipe\n\nEven though the elastic net regression was only slightly better, I decided to update the workflow using that model. This time I decided to update the recipe by including additional predictors like if there were any defects in the green coffee beans, the species of the coffee (e.g., Robusta and Arabica), and the country of origin. I also included additional steps in my recipe by transforming the category predictors and working with the factor predictors, like species, and country of origin. The inclusion of additional steps and the predictors created a better fitting model with the elastic net regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05132021)\n\nbal_rec <- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points + category_one_defects + category_two_defects + species +\n                        country_of_origin,\n                      data = coffee_train) %>% \n  step_BoxCox(category_two_defects, category_one_defects) %>% \n  step_novel(species, country_of_origin) %>% \n  step_other(species, country_of_origin, threshold = .01) %>%\n  step_unknown(species, country_of_origin) %>% \n  step_dummy(species, country_of_origin) %>% \n  step_zv(all_predictors(), -all_outcomes())\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n! Fold01: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold02: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold03: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold04: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold05: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold06: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold07: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold08: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold09: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold10: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(elastic_bal_fit) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 300 x 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>       \n 1 0.0000000001        0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n 2 0.0000000001        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso~\n 3 0.0000000001        0 roc_auc     binary     0.840    10 0.0163  Preprocesso~\n 4 0.00000000129       0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n 5 0.00000000129       0 mn_log_loss binary     0.431    10 0.0153  Preprocesso~\n 6 0.00000000129       0 roc_auc     binary     0.840    10 0.0163  Preprocesso~\n 7 0.0000000167        0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n 8 0.0000000167        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso~\n 9 0.0000000167        0 roc_auc     binary     0.840    10 0.0163  Preprocesso~\n10 0.000000215         0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n# ... with 290 more rows\n# i Use `print(n = ...)` to see more rows\n```\n:::\n\n```{.r .cell-code}\nshow_best(elastic_bal_fit, metric = \"accuracy\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1 0.000464   0.111 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0~\n2 0.000464   0.222 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0~\n3 0.00599    0.111 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0~\n4 0.00599    0.222 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0~\n5 0.00599    0.333 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0~\n```\n:::\n\n```{.r .cell-code}\nshow_best(elastic_bal_fit, metric = \"mn_log_loss\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 8\n   penalty mixture .metric     .estimator  mean     n std_err .config           \n     <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>             \n1 0.000464   1     mn_log_loss binary     0.420    10  0.0179 Preprocessor1_Mod~\n2 0.000464   0.889 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod~\n3 0.000464   0.778 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod~\n4 0.000464   0.667 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod~\n5 0.000464   0.556 mn_log_loss binary     0.420    10  0.0177 Preprocessor1_Mod~\n```\n:::\n\n```{.r .cell-code}\nshow_best(elastic_bal_fit, metric = \"roc_auc\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 8\n   penalty mixture .metric .estimator  mean     n std_err .config               \n     <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 \n1 0.00599    0.778 roc_auc binary     0.843    10  0.0150 Preprocessor1_Model078\n2 0.000464   0.667 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model067\n3 0.00599    0.889 roc_auc binary     0.843    10  0.0148 Preprocessor1_Model088\n4 0.000464   0.444 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model047\n5 0.000464   0.556 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model057\n```\n:::\n\n```{.r .cell-code}\nselect_best(elastic_bal_fit, metric = \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n   penalty mixture .config               \n     <dbl>   <dbl> <chr>                 \n1 0.000464   0.111 Preprocessor1_Model017\n```\n:::\n\n```{.r .cell-code}\nselect_best(elastic_bal_fit, metric = \"mn_log_loss\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n   penalty mixture .config               \n     <dbl>   <dbl> <chr>                 \n1 0.000464       1 Preprocessor1_Model097\n```\n:::\n\n```{.r .cell-code}\nselect_best(elastic_bal_fit, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  penalty mixture .config               \n    <dbl>   <dbl> <chr>                 \n1 0.00599   0.778 Preprocessor1_Model078\n```\n:::\n:::\n\n\nNow using the testing dataset, we can see how well the final model fit the testing data. While not the best at predicting washed green coffee beans, this was a good test to show that the penalized regressions are not always the best fitting models compared to regular logistic regression. In the end, it seemed like the recipe was the most important component to predicting washed green coffee beans.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n! train/test split: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_results %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.823 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}